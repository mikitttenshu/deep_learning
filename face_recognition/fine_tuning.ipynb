{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import chainer\n",
    "from chainer import cuda, Variable, FunctionSet, optimizers, serializers ### Add 'serializers'\n",
    "import chainer.functions  as F\n",
    "import chainer.links as L\n",
    "import pdb\n",
    "import argparse\n",
    "from Original_VGGNet import Original_VGGNet\n",
    "import glob\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import cPickle as pickle\n",
    "import cv2\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--gpu',default=0,help='GPU ID (default 0)')\n",
    "#parser.add_argument('--model',default=0,help='use pre-trained model or not')\n",
    "#parser.add_argument('--load',default=1,help='LOAD or not (default 1)')\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "classes =  [\"aragaki_face\",\"hoshino_face\",\"narita_face\",\"other_face\",\"fujii_face\",\"mano_face\",\"ohtani_face\",\"yamaga_face\",\"ishida_face\"]\n",
    "#load=int(args.load)\n",
    "\n",
    "class VGGNet(chainer.Chain):\n",
    "\n",
    "    \"\"\"\n",
    "    VGGNet\n",
    "    - It takes (224, 224, 3) sized image as imput\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__(\n",
    "            conv1_1=L.Convolution2D(3, 64, 3, stride=1, pad=1),\n",
    "            conv1_2=L.Convolution2D(64, 64, 3, stride=1, pad=1),\n",
    "\n",
    "            conv2_1=L.Convolution2D(64, 128, 3, stride=1, pad=1),\n",
    "            conv2_2=L.Convolution2D(128, 128, 3, stride=1, pad=1),\n",
    "\n",
    "            conv3_1=L.Convolution2D(128, 256, 3, stride=1, pad=1),\n",
    "            conv3_2=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "            conv3_3=L.Convolution2D(256, 256, 3, stride=1, pad=1),\n",
    "\n",
    "            conv4_1=L.Convolution2D(256, 512, 3, stride=1, pad=1),\n",
    "            conv4_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv4_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "\n",
    "            conv5_1=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv5_2=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "            conv5_3=L.Convolution2D(512, 512, 3, stride=1, pad=1),\n",
    "\n",
    "            fc6=L.Linear(7*7*512, 4096),\n",
    "            fc7=L.Linear(4096, 4096),\n",
    "            fc8=L.Linear(4096,9)\n",
    "            # the output dim '10' is set up to CIFAR-10. Please change this.\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, t,train):\n",
    "        h = F.relu(self.conv1_1(x))\n",
    "        h = F.relu(self.conv1_2(h))\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv2_1(h))\n",
    "        h = F.relu(self.conv2_2(h))\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv3_1(h))\n",
    "        h = F.relu(self.conv3_2(h))\n",
    "        h = F.relu(self.conv3_3(h))\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv4_1(h))\n",
    "        h = F.relu(self.conv4_2(h))\n",
    "        h = F.relu(self.conv4_3(h))\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv5_1(h))\n",
    "        h = F.relu(self.conv5_2(h))\n",
    "        h = F.relu(self.conv5_3(h))\n",
    "        h = F.max_pooling_2d(h, 2, stride=2)\n",
    "\n",
    "        h = F.dropout(F.relu(self.fc6(h)), train=train, ratio=0.5)\n",
    "        h = F.dropout(F.relu(self.fc7(h)), train=train, ratio=0.5)\n",
    "\n",
    "        h = self.fc8(h)\n",
    "\n",
    "        self.loss = F.softmax_cross_entropy(h, t)\n",
    "        self.acc = F.accuracy(h, t)\n",
    "        self.pred = F.softmax(h)\n",
    "        return self.loss,self.acc,self.pred\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def conv_setup(ORIGINAL_VGG,VGG):\n",
    "    VGG.conv1_1 = ORIGINAL_VGG.conv1_1\n",
    "    VGG.conv1_2 = ORIGINAL_VGG.conv1_2\n",
    "    VGG.conv2_1 = ORIGINAL_VGG.conv2_1\n",
    "    VGG.conv2_2 = ORIGINAL_VGG.conv2_2\n",
    "    VGG.conv3_1 = ORIGINAL_VGG.conv3_1\n",
    "    VGG.conv3_2 = ORIGINAL_VGG.conv3_2\n",
    "    VGG.conv3_3 = ORIGINAL_VGG.conv3_3\n",
    "    VGG.conv4_1 = ORIGINAL_VGG.conv4_1\n",
    "    VGG.conv4_2 = ORIGINAL_VGG.conv4_2\n",
    "    VGG.conv4_3 = ORIGINAL_VGG.conv4_3\n",
    "    VGG.conv5_1 = ORIGINAL_VGG.conv5_1\n",
    "    VGG.conv5_2 = ORIGINAL_VGG.conv5_2\n",
    "    VGG.conv5_3 = ORIGINAL_VGG.conv5_3\n",
    "    VGG.fc6=ORIGINAL_VGG.fc6\n",
    "    VGG.fc7=ORIGINAL_VGG.fc7\n",
    "    return VGG\n",
    "\n",
    "\n",
    "def preprocess(x_train, x_test):\n",
    "    print(x_train.shape)\n",
    "    mean = np.mean(x_train, axis=(0, 2, 3), keepdims=True)\n",
    "    np.save(\"mean_face.npy\",mean)\n",
    "    return x_train-mean, x_test-mean\n",
    "\n",
    "def augment(x,size):#4D tensor\n",
    "    #flip = np.random.randint(2,size = len(x))*2-1\n",
    "    theta = np.random.uniform(-0.1*np.pi,0.1*np.pi,len(x))\n",
    "    scale = np.random.uniform(0.9,1.2,len(x)).reshape(-1,1,1)\n",
    "    shift = np.random.uniform(-5,5,len(x)*2).reshape(-1,2)\n",
    "    xs = np.arange(size**2)%size\n",
    "    ys = np.arange(size**2)/size\n",
    "    coords = np.c_[xs,ys].transpose()-size/2.#変換前の座標\n",
    "    R = scale*(np.c_[np.cos(theta),-np.sin(theta),np.sin(theta),np.cos(theta)].reshape(len(x),2,2))\n",
    "    img = np.array([X[:,np.clip(np.dot(r,coords)[1]+size/2.+s[1],0,size-1).astype('int32'),np.clip(np.dot(r,coords)[0]+size/2.+s[0],0,size-1).astype('int32')].reshape(3,size,size) for s,r,X in zip(shift,R,x)])\n",
    "    return img\n",
    "\n",
    "def load_faces():\n",
    "    data_dir = \"data/faces_nparray/\"\n",
    "    classes =  [\"aragaki_face\",\"hoshino_face\",\"narita_face\",\"other_face\",\"fujii_face\",\"mano_face\",\"ohtani_face\",\"yamaga_face\",\"ishida_face\"]\n",
    "    X=[]\n",
    "    y=[]\n",
    "    \n",
    "    for i, cls in enumerate(classes):\n",
    "        X_=np.load(data_dir+cls+\".npy\").astype(np.uint8)\n",
    "        kkk=np.array([cv2.resize(ll,((224,224))).transpose(2,0,1)/255. for ll in X_])\n",
    "        X.extend(kkk)\n",
    "        y.extend(np.ones(len(kkk),dtype=np.int32)*i)\n",
    "    X = np.array(X,dtype=np.float32)\n",
    "    y = np.array(y,dtype=np.int32)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_train():\n",
    "    with open('vgg.pkl', 'rb') as i:\n",
    "        orig_vgg = pickle.load(i)\n",
    "    vgg=VGGNet()\n",
    "    #if int(args.model) == 1:\n",
    "    #orig_vgg = Original_VGGNet()\n",
    "    #serializers.load_hdf5('VGG.model', orig_vgg)\n",
    "    vgg = conv_setup(orig_vgg,vgg)\n",
    "    print 'loading data now.'\n",
    "    \n",
    "    #if not load :\n",
    "    X,y = load_faces()\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print 'loading data done'\n",
    "    x_train,x_test = preprocess(x_train,x_test)\n",
    "    batchsize = 5\n",
    "    N = len(x_train)\n",
    "    N_test = len(x_test)\n",
    "    n_epoch = 50\n",
    "    #gpu = int(args.gpu)\n",
    "    optimizer = optimizers.MomentumSGD(lr=0.002,momentum=0.9)\n",
    "    \n",
    "    optimizer.setup(vgg)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(0.001))\n",
    "    vgg.to_gpu()\n",
    "    \n",
    "    for epoch in xrange(1, n_epoch+1):\n",
    "        \n",
    "        if epoch in [35,45]:\n",
    "            optimizer.lr*=0.20\n",
    "        print 'epoch', epoch\n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_loss = 0\n",
    "        for i in xrange(0, N, batchsize):\n",
    "            x_batch = cuda.to_gpu(augment(x_train[perm[i:i+batchsize]],224))\n",
    "            y_batch = cuda.to_gpu(y_train[perm[i:i+batchsize]])\n",
    "            optimizer.zero_grads()\n",
    "            loss,_,_ = vgg(x_batch, y_batch, train=True)\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            sum_loss     += float(cuda.to_cpu(loss.data)) * len(x_batch)\n",
    "\n",
    "        print 'train mean loss={}'.format(sum_loss / N)\n",
    "        \n",
    "        # evaluation\n",
    "        sum_accuracy = 0\n",
    "        \n",
    "        pred_y =[]\n",
    "        for i in xrange(0, N_test, batchsize):\n",
    "            x_batch = cuda.to_gpu(x_test[i:i+batchsize])\n",
    "            y_batch = cuda.to_gpu(y_test[i:i+batchsize])\n",
    "\n",
    "            _,acc,pred = vgg(x_batch,y_batch,train=False)\n",
    "            pred_y.extend(np.argmax(cuda.to_cpu(pred.data),axis=1))\n",
    "            sum_accuracy += float(cuda.to_cpu(acc.data)) * len(x_batch)\n",
    "\n",
    "        print 'test mean accuracy={}'.format(sum_accuracy / N_test)\n",
    "        for i in range(len(classes)):\n",
    "            accuracy = np.sum((np.array(pred_y)==y_test)*(y_test==i))*1./np.sum(y_test==i)\n",
    "            print '    {} accuracy={}'.format(classes[i],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with open('vgg.pkl', 'rb') as i:\n",
    "        orig_vgg = pickle.load(i)\n",
    "    vgg=VGGNet()\n",
    "    #if int(args.model) == 1:\n",
    "    #orig_vgg = Original_VGGNet()\n",
    "    #serializers.load_hdf5('VGG.model', orig_vgg)\n",
    "    vgg = conv_setup(orig_vgg,vgg)\n",
    "    print 'loading data now.'\n",
    "    \n",
    "    #if not load :\n",
    "    X,y_train = load_faces()\n",
    "    \n",
    "    print 'loading data done'\n",
    "    x_train,_= preprocess(X,[])\n",
    "    batchsize = 5\n",
    "    N = len(x_train)\n",
    "    n_epoch = 50\n",
    "    #gpu = int(args.gpu)\n",
    "    optimizer = optimizers.MomentumSGD(lr=0.002,momentum=0.9)\n",
    "    \n",
    "    optimizer.setup(vgg)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(0.001))\n",
    "    vgg.to_gpu()\n",
    "    \n",
    "    for epoch in xrange(1, n_epoch+1):\n",
    "        \n",
    "        if epoch in [35,45]:\n",
    "            optimizer.lr*=0.20\n",
    "            \n",
    "        print 'epoch', epoch\n",
    "        # training\n",
    "        perm = np.random.permutation(N)\n",
    "        sum_loss = 0\n",
    "        for i in xrange(0, N, batchsize):\n",
    "            x_batch = cuda.to_gpu(augment(x_train[perm[i:i+batchsize]],224))\n",
    "            y_batch = cuda.to_gpu(y_train[perm[i:i+batchsize]])\n",
    "            optimizer.zero_grads()\n",
    "            loss,_,_ = vgg(x_batch, y_batch, train=True)\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "            sum_loss     += float(cuda.to_cpu(loss.data)) * len(x_batch)\n",
    "\n",
    "        print 'train mean loss={}'.format(sum_loss / N)\n",
    "        \n",
    "    serializers.save_hdf5('VGGface_{}.model'.format(str(sum_loss/N).replace('.','')), vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_testFaces():\n",
    "    dirs = [\"aragaki\",\"hoshino\",\"narita\",\"furuta\",\"fujii\",\"mano\",\"ohtani\",\"yamaga\",\"ishida\"]\n",
    "    #X = np.empty((0, 3, 224, 224))\n",
    "    X = []\n",
    "    y = []\n",
    "    for i,dir in enumerate(dirs):\n",
    "        data_dir=\"./data/kaokai/\"+dir+\"/\"\n",
    "        for f in sorted(glob.glob(data_dir+\"/*.jpg\")):\n",
    "            x = np.array(Image.open(f).resize((224, 224))) \n",
    "            x = np.transpose(x, (2, 0, 1)) / 255. \n",
    "            X.append(x)\n",
    "            y.append(i)\n",
    "    y=np.array(y,dtype=np.int32)\n",
    "    X=np.array(X,dtype=np.float32)\n",
    "    return X,y\n",
    "\n",
    "def test():\n",
    "    model=VGGNet()\n",
    "    model.to_gpu()\n",
    "    serializers.load_hdf5(\"VGGface_00010659227118.model\",model)\n",
    "    #if int(args.model) == 1:\n",
    "    #orig_vgg = Original_VGGNet()\n",
    "    #serializers.load_hdf5('VGG.model', orig_vgg)\n",
    "    #vgg = conv_setup(orig_vgg,vgg)\n",
    "    print 'loading data now.'\n",
    "    #if not load :\n",
    "    X,y = load_testFaces()\n",
    "    print \"data loaded\"\n",
    "    N_test=len(y)\n",
    "    mean=np.load(\"mean_face.npy\")\n",
    "    X-=mean\n",
    "    batch_num=10\n",
    "    pred=[]\n",
    "    for i in range(0,N_test,batch_num):\n",
    "        x=Variable(cuda.to_gpu(X[i:i+batch_num]))\n",
    "        t=Variable(cuda.to_gpu(y[i:i+batch_num]))\n",
    "        _,_,pre=model(x,t,train=False)\n",
    "        pred.extend(cuda.to_cpu(pre.data))\n",
    "    classes=[\"aragaki\",\"hoshino\",\"narita\",\"furuta\",\"fujii\",\"mano\",\"ohtani\",\"yamaga\",\"ishida\"]\n",
    "    prediction=np.argmax(pred,axis=1)\n",
    "    print(prediction)\n",
    "    accuracy = np.sum(np.eye(10)[prediction] * np.eye(10)[y]) / N_test\n",
    "    print(accuracy)\n",
    "    indices=[]\n",
    "    for i in range(len(classes)):\n",
    "        print(classes[i]+\"accuracy : \"+str(float(np.sum((y==i)*(prediction==i)))/np.sum(y==i)*100)+\"%\")\n",
    "        indices.append(list(set(np.where(y==i)[0]).difference(set(np.where((y==i)*(prediction==i))[0]))))\n",
    "    np.save(\"indices.npy\",np.array(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mil/tanaka/.pyenv/versions/anaconda2-4.0.0/lib/python2.7/site-packages/chainer/cuda.py:90: UserWarning: cuDNN is not enabled.\n",
      "Please reinstall chainer after you install cudnn\n",
      "(see https://github.com/pfnet/chainer#installation).\n",
      "  'cuDNN is not enabled.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data now.\n",
      "data loaded\n",
      "[0 0 0 ..., 0 5 5]\n",
      "0.786353944563\n",
      "aragakiaccuracy : 85.4545454545%\n",
      "hoshinoaccuracy : 98.8235294118%\n",
      "naritaaccuracy : 96.9696969697%\n",
      "furutaaccuracy : 93.2835820896%\n",
      "fujiiaccuracy : 100.0%\n",
      "manoaccuracy : 73.3333333333%\n",
      "ohtaniaccuracy : 96.6666666667%\n",
      "yamagaaccuracy : 89.2857142857%\n",
      "ishidaaccuracy : 36.3449691992%\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y_train = load_faces()\n",
    "mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n",
    "np.save(\"mean.npy\",mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [1037, 1038, 1044, 686, 1046, 1047, 539, 858, 945, 861, 587, 605, 612, 617, 618, 619, 620, 621, 622, 623, 624, 625, 628, 629, 123, 124, 639, 640, 642, 643, 649, 650, 651, 993, 666, 1045, 673, 674, 174, 687, 689, 194, 220, 741, 752, 758, 759, 760, 251, 252, 258, 259, 261, 262, 263, 264, 265, 783, 795, 817, 315, 316, 833, 834, 835, 837, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 377, 381, 385, 386, 389, 390, 391, 392, 394, 395, 397, 410, 923, 412, 925, 416, 419, 420, 422, 935, 424, 924, 938, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 441, 442, 444, 842, 446, 447, 961, 931, 932, 848, 934, 954, 936, 1021]\n",
      " [1152, 1195, 1277, 1238] [1445]\n",
      " [1571, 1572, 1573, 1604, 1589, 1561, 1596, 1597, 1598] []\n",
      " [1632, 1645, 1646, 1647, 1648, 1649, 1629, 1631]\n",
      " [1821, 1657, 1658, 1819, 1820, 1818] [1841, 1843, 1857]\n",
      " [2048, 2049, 2050, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2074, 2075, 2076, 2077, 2078, 2079, 2082, 2083, 2084, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2180, 2198, 2210, 2216, 2217, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2255, 2261, 2262, 2263, 2264, 2265, 2266, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2295, 2296, 2297, 2298, 2299, 2302, 2303, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 1859, 1865, 1866, 1867, 1868, 1869, 1874, 1875, 1876, 1886, 1887, 1888, 1894, 1898, 1900, 1901, 1902, 1903, 1904, 1905, 1911, 1912, 1913, 1930, 1931, 1932, 1934, 1935, 1937, 1945, 1953, 1954, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 2000, 2002, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2017, 2018, 2019, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2034, 2036, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047]]\n"
     ]
    }
   ],
   "source": [
    "indices=np.load(\"indices.npy\")\n",
    "print(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
